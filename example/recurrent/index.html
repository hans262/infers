<html>

<head>
  <title>RecurrentJS</title>
  <style>
    body {
      padding: 20px;
    }

    #container {
      max-width: 1000px;
      margin: 0 auto;
    }

    .hh {
      background-color: #EEE;
      padding: 5px;
      margin-top: 5px;
      border-bottom: 1px solid #999;
      margin-bottom: 2px;
    }

    .abutton {
      width: 120px;
      height: 30px;
      margin: 10px 10px 10px 0px;
    }

    #argmax {
      color: white;
      background-color: #318631;
    }

    #epoch {
      color: #090;
    }
  </style>
  <script src="./recurrent.js"></script>
  <script src="./vis.js"></script>
</head>

<body>
  <div id="container">
    <div class="hh">Input text:</div>
    <div id="prepro_status"></div>
    <button id="learn" class="abutton">learn/restart</button>
    <button id="resume" class="abutton">resume</button>
    <button id="stop" class="abutton">pause</button>
    <button id="predict" class="abutton">predict</button>
    <div class="hh">Training stats:</div>
    <canvas id="pplgraph"></canvas>
    <div id="epoch"></div>
    <div class="hh">Model Sample:</div>
    <div id="samples"></div>
    <div class="hh">Predict:</div>
    <div id="argmax"></div>
  </div>
  <script type="text/javascript">
    // 模型参数
    let generator = 'rnn'; // 'rnn' | 'lstm'
    let hidden_sizes = [20, 20]; // hidden layers
    let letter_size = 5; // size of letter embeddings

    // 优化
    let regc = 0.000001; // L2 正则惩罚
    let learning_rate = 0.001; // learning rate
    let clipval = 5.0; // 梯度

    // 预测参数
    var sample_softmax_temperature = 1.0; // 预测峰值
    var max_chars_gen = 100; // 生成句子的最大长度

    // 全局变量初始化
    var epoch_size = -1
    var input_size = -1
    var output_size = -1
    var letterToIndex = {}
    var indexToLetter = {}
    var vocab = []
    var data_sents = ['无论即将到来的是大数据时代还是人工智能时代',
      '亦或是传统行业使用人工智能在云上处理大数据的时代',
      '作为一个有理想有追求的程序员',
      '不懂深度学习这个超热的技术',
      '会不会感觉马上就落后了']
    var solver = new R.Solver()
    var pplGraph = new Rvis.Graph()
    var model = {}

    function initVocab(sents, count_threshold) {
      // go over all characters and keep track of all unique ones seen
      var txt = sents.join(''); // concat all

      // count up all characters
      var d = {};
      for (var i = 0, n = txt.length; i < n; i++) {
        var txti = txt[i];
        if (txti in d) { d[txti] += 1; }
        else { d[txti] = 1; }
      }

      // filter by count threshold and create pointers
      letterToIndex = {};
      indexToLetter = {};
      vocab = [];
      // NOTE: start at one because we will have START and END tokens!
      // that is, START token will be index 0 in model letter vectors
      // and END token will be index 0 in the next character softmax
      var q = 1;
      for (ch in d) {
        if (d.hasOwnProperty(ch)) {
          if (d[ch] >= count_threshold) {
            // add character to vocab
            letterToIndex[ch] = q;
            indexToLetter[q] = ch;
            vocab.push(ch);
            q++;
          }
        }
      }

      // globals written: indexToLetter, letterToIndex, vocab (list), and:
      input_size = vocab.length + 1;
      output_size = vocab.length + 1;
      epoch_size = sents.length;
    }

    function utilAddToModel(modelto, modelfrom) {
      for (var k in modelfrom) {
        if (modelfrom.hasOwnProperty(k)) {
          // copy over the pointer but change the key to use the append
          modelto[k] = modelfrom[k];
        }
      }
    }

    function initModel() {
      // letter embedding vectors
      var model = {};
      model['Wil'] = new R.RandMat(input_size, letter_size, 0, 0.08);

      if (generator === 'rnn') {
        var rnn = R.initRNN(letter_size, hidden_sizes, output_size);
        utilAddToModel(model, rnn);
      } else {
        var lstm = R.initLSTM(letter_size, hidden_sizes, output_size);
        utilAddToModel(model, lstm);
      }

      return model;
    }

    function reinit() {
      solver = new R.Solver() // reinit solver
      pplGraph = new Rvis.Graph()

      ppl_list = []
      tick_iter = 0

      initVocab(data_sents, 1); // takes count threshold for characters
      model = initModel();
    }

    function forwardIndex(G, model, ix, prev) {
      var x = G.rowPluck(model['Wil'], ix);
      // forward prop the sequence learner

      if (generator === 'rnn') {
        var out_struct = R.forwardRNN(G, model, hidden_sizes, x, prev);
      } else {
        var out_struct = R.forwardLSTM(G, model, hidden_sizes, x, prev);
      }
      return out_struct;
    }

    function predictSentence(model, samplei) {
      var G = new R.Graph(false)
      var s = ''
      var prev = {}
      while (true) {
        var ix = s.length === 0 ? 0 : letterToIndex[s[s.length - 1]];
        var lh = forwardIndex(G, model, ix, prev);
        prev = lh
        let probs = R.softmax(lh.o)
        if (samplei) {
          var ix = R.samplei(probs.w)
        } else {
          var ix = R.maxi(probs.w)
        }
        if (ix === 0) break
        if (s.length > max_chars_gen) break
        var letter = indexToLetter[ix]
        s += letter
      }
      return s
    }


    function costfun(model, sent) {
      // takes a model and a sentence and
      // calculates the loss. Also returns the Graph
      // object which can be used to do backprop
      var n = sent.length;
      var G = new R.Graph();
      var log2ppl = 0.0;
      var cost = 0.0;
      var prev = {};
      for (var i = -1; i < n; i++) {
        // start and end tokens are zeros
        var ix_source = i === -1 ? 0 : letterToIndex[sent[i]]; // first step: start with START token
        var ix_target = i === n - 1 ? 0 : letterToIndex[sent[i + 1]]; // last step: end with END token

        lh = forwardIndex(G, model, ix_source, prev);
        prev = lh;

        // set gradients into logprobabilities
        logprobs = lh.o; // interpret output as logprobs
        probs = R.softmax(logprobs); // compute the softmax probabilities

        log2ppl += -Math.log2(probs.w[ix_target]); // accumulate base 2 log prob and do smoothing
        cost += -Math.log(probs.w[ix_target]);

        // write gradients into log probabilities
        logprobs.dw = probs.w;
        logprobs.dw[ix_target] -= 1
      }
      var ppl = Math.pow(2, log2ppl / (n - 1));
      return { 'G': G, 'ppl': ppl, 'cost': cost };
    }

    function median(list) {
      return list.reduce((a, b) => a + b) / list.length
    }

    var ppl_list = []
    var tick_iter = 0

    function tick() {
      // sample sentence fromd data
      var sentix = R.randi(0, data_sents.length);
      var sent = data_sents[sentix];

      // evaluate cost function on a sentence
      var cost_struct = costfun(model, sent);

      // use built up graph to compute backprop (set .dw fields in mats)
      cost_struct.G.backward();
      // perform param update
      var solver_stats = solver.step(model, learning_rate, regc, clipval);

      ppl_list.push(cost_struct.ppl); // keep track of perplexity

      // evaluate now and then
      tick_iter += 1;
      if (tick_iter % 50 === 0) {
        let samples = document.getElementById('samples')
        samples.innerHTML = ''
        for (var q = 0; q < 5; q++) {
          var pred = predictSentence(model, true)
          var pred_div = document.createElement('div')
          pred_div.innerHTML = pred
          samples.appendChild(pred_div)
        }
      }

      if (tick_iter % 10 === 0) {
        var pred = predictSentence(model)
        document.getElementById('argmax').innerHTML = pred

        // keep track of perplexity
        document.getElementById('epoch').innerHTML = 'epoch: ' + (tick_iter / epoch_size).toFixed(2) + ' perplexity: ' + cost_struct.ppl.toFixed(2)

        if (tick_iter % 100 === 0) {
          var median_ppl = median(ppl_list);
          ppl_list = [];
          pplGraph.add(tick_iter, median_ppl);
          pplGraph.drawSelf(document.getElementById("pplgraph"));
        }
      }
    }

    document.getElementById('prepro_status').innerHTML = data_sents.join('<br/>')

    let prev = {}
    var s = ''

    document.getElementById('predict').onclick = () => {

      // RNN tick
      var ix = s.length === 0 ? 0 : letterToIndex[s[s.length - 1]]
      var lh = forwardIndex(new R.Graph(false), model, ix, prev)
      prev = lh

      logprobs = lh.o
      probs = R.softmax(logprobs)
      var ix = R.maxi(probs.w)

      var letter = indexToLetter[ix]
      if (ix === 0) {
        console.log('结尾')
        return
      }
      s += letter

      console.log(s)
    }


    var iid = null
    document.getElementById('learn').onclick = () => {
      reinit()
      if (iid) { clearInterval(iid) }
      iid = setInterval(tick, 10)
    }
    document.getElementById('resume').onclick = () => {
      if (!iid) {
        iid = setInterval(tick, 10)
      }
    }
    document.getElementById('stop').onclick = () => {
      if (iid) { clearInterval(iid) }
      iid = null
    }
  </script>
</body>

</html>