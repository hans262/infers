机器学习 Machine Learning - by 2021.02
======================================
机器学习这个词在几年前确实有点超前，人们往往无法把它和生活所结合起来，但在当今社会如果你是一个细心观察的人，你会发现机器学习已经无处不在。
例如：
- google的新闻聚类，google每天收集大量的新闻，对具有相同特征的新闻进行分簇；
- 游戏中对非常坑的队友人身攻击，会被文字模型预测为人身攻击，从而禁止你输出；
- 直播平台的弹幕自动被视频当前人像隐藏，利用人像关键点识别，即可抓取人像出现的坐标点；
- 等等～

机器学习是人工智能的一个发展方向，机器学习和深度学习都属于人工智能的一个子集。

机器学习分为以下三种方式：
- 监督学习
- 无监督学习
- 强化学习

## What is Machine Learning?
Arthur Samuel (1959): 在没有明确设定的情况下，使计算机具有学习能力的研究领域。

Tom Mitchell (1998): 计算机程序从经验E中学习，解决某一任务T进行某一性能度量P，通过P测定在T上的表现因经验E而提高。

李宏毅：机器学习就是让机器自己找函式。往往这样的函式我们很难实现，这时候就可以通过机器学习给我找出函式。
- function(音频) -> 文字
- function(图片) -> 分类

## 监督学习 Supervised learning
根据已有的数据集，已知输入和输出结果之间的关系，根据这种关系训练得到一个最优的模型，该过程也就是监督学习。也就是说，在监督学习中训练数据既有特征（输入）又有标签（输出），通过训练学习，让机器可以自己找出特征和标签之间的联系，在面对只有特征没有标签的数据时，可以判断初标签。

监督学习的分类：
- **回归（Regression）**：准确来说就是对训练数据进行分析，拟合出适当的函数模型y=f(x)，回归问题多用来预测一个具体的值。
- **分类（Classification）**：分类问题一般用来预测离散值输出，输出是有限的。例如根据特征预测男女，预测衣服尺寸，预测肿瘤良性还是恶性。

<*>人脸识别是分类还是回归问题？人脸识别是一个分类问题。


## 无监督学习 Unsupervised learning
当缺乏足够的先验知识，难以人工的标注特征和标签或进行人工标注的成本太高，我们希望计算机来完成这些工作。也就是说，根据没有标签的训练数据来进行训练，称之为无监督学习。

无监督学习典型的例子是聚类。其目的是把相似的东西聚在一起，我们不关心这一类是什么，因此聚类算法通常只需要知道如何计算相似度就可以开始工作了。

- google的新闻整合，对大量的新闻进行分簇，把具有相似特征的新闻分到一起。
- 根据给定基因数据，对DNA数据进行分簇。

## 线性回归模型 Linear Regression
在数学中，表示两种或两种以上变量间相互依赖的关系，其表达式为y = wx + b，b为误差服从均值为0的正态分布。

在机器学习中，通过一组训练数据根据特征和标签之间的关系来拟合一条直线，该直线就是一个线性回归模型，通过该模型可以根据特征预测其标签。

用一元二次方程表示 -> y = w*x + b，自变量y随因变量x的改变而改变，通过大量的已知数据{x, y}求导w和b的值，当我们输入一个新的x时能预测其y。

怎么才能让表达式更加准确呢，也就是说让该模型的预测越来越准确，我们需要计算每次输出结果的损失，当损失的均值越来越趋近于0的时候，我们也就可以说该模型已经是理想状态最好的了。


## 假设函数
机器学习最终用来预测目标特征的输出函数，用数学表达式可表示为：

H(x) = θ0 + Θ1 * X

这也是一个简单的线性回归模型，该模型中又两个模型参数(θ0, θ1)，是最简单的线性回归模型。


## 代价函数/损失函数
代价函数又称损失函数，通常用于模型参数估计，通过最小化损失函数求解来评估模型，以达到最优解模型。

常用的损失函数有：
- 绝对值损失函数：L(Y, F(x)) = |Y - F(x)|
- 平方损失函数：<br/>![avatar](./glow/draftdoc/images/loss.png)<br/>
该公式也是公认最符合机器学习的代价函数，其中m为训练样本个数，hθ(x[i])表示假设函数，y[i]表示对应标签。

## 梯度下降法
一种按照步长迭代的方式来寻找代价函数最小值求解，并不断修改模型权重参数，越靠近极小值时收敛速度减慢，这样的过程称之为梯度下降法。

假设一线性回归模型有特征参数{X1, X2, ..., Xn}，则有模型参数{θ0, θ1, θ2, ..., θn}，其假设函数为：  
`H(x) = θ0 + Θ1 * X1 + Θ2 * X2 + ... + Θn * Xn`

则有以下公式来求解下一次梯度的权重参数值：
```ts
//这里我们设 X0[i] = 1 因为没有这个参数
//学习率用来调节梯度大小
let rate = 0.001
θ0 = θ0 - rate * 1/m * ∑m(H(X[i]) - Y[i]) * X0[i]
θ1 = θ0 - rate * 1/m * ∑m(H(X[i]) - Y[i]) * X1[i]
θ2 = θ0 - rate * 1/m * ∑m(H(X[i]) - Y[i]) * X2[i]
```

## 贝叶斯定理
是关于随机事件A和B的条件概率，在B发生的情况下A发生的可能性记做条件概率P(A|B)。使用以下例子阐述贝叶斯公式：

一座别墅在20年内被盗过2次，该别墅里的狗每周叫3次，已知盗贼入侵时狗叫的概率为0.9，求狗叫的时候盗贼入侵的概率？
```ts
P(A) = 3 / 7                   //狗叫的概率
P(B) = 2 / 20 * 365            //盗贼入侵的概率
P(A|B) = 0.9                   //盗贼入侵时候狗叫的概率
P(B|A) = P(A|B) * P(B) / P(A)  //狗叫的时候盗贼入侵的概率
P(A∩B) = P(A|B) * P(B)         //盗贼入侵狗叫同时发生的概率
```
### 条件概率
即当一个事件发生时另一个事件发生的概率 -> 当盗贼入侵(B)时狗叫(A)的概率 -> P(A|B)
### 先验概率/后验概率
- 先验概率不是根据有关自然状态的全部资料测定的，而只是利用现有的材料(主要是历史资料)计算的；
- 后验概率使用了有关自然状态更加全面的资料，既有先验概率资料，也有补充资料；
- 上面的别墅被盗问题中P(A)、P(B)都是先验概率，他们都是基于历史数据的测定；

### 联合概率
表示两个随机事件同时发生的概率 -> P(A∩B) 或 P(AB)
