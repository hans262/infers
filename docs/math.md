# math

## 方差
衡量一组数据离散程度的度量，公式为S = ∑(X - μ)^2 / N。
```ts
  //给出五次成绩，求方差？
  let a = [50 100 100 60 50]
  let N = 5
  let μ = (50 + 100 + 100 + 60 + 50) / N
  S = ((50 - μ)^2 + (100 - μ)^2 + (100 - μ)^2 + (60 - μ)^2 + (50 - μ)^2) / N
```

## 特征缩放
用通俗的话说，就是把一组数据缩放到一段值域空间，将一组数据映射到0～1之间的这个过程就叫做特征缩放。

机器学习中特征缩放往往可以加速模型收敛速度和训练过程的平稳性。

特征缩放的方式：
- Min-Max Normalization(min-max标准化):  

```
x' = (x - min) / (max - min)
# 将结果映射到0~1之间

average = min + (range / 2)
x' = (x - average) / (max - min)
# 将结果映射到-0.5~0.5之间
```


## 对数
在数学中，对数是对求幂的逆运算，对数是相对于另两个数字而言的。

如果![avatar](./glow/draftdoc/images/log.svg)，即a的x次方等于N，那么x叫做以a为底N的对数，记做x = loga N，a叫做对数的底数，N叫做真数。

- 以10为底的对数叫做常用对数，记做lgN；
- 以e（e=2.71828…）为底的对数称为自然对数，记为lnN；
- 零没有对数；
- 在实数范围内，负数无对数。在虚数范围内，负数是有对数的；

## 求和符号Σ
Σ是一个求和符号，英文名称Sigma，（大写Σ，小写σ）。<br/>
![avatar](./glow/draftdoc/images/sigma2.jpg)

在程序中可以理解为，其中下标为索引，上标为索引长度。


## 贝叶斯定理
是关于随机事件A和B的条件概率，在B发生的情况下A发生的可能性记做条件概率P(A|B)。使用以下例子阐述贝叶斯公式：

一座别墅在20年内被盗过2次，该别墅里的狗每周叫3次，已知盗贼入侵时狗叫的概率为0.9，求狗叫的时候盗贼入侵的概率？
```ts
P(A) = 3 / 7                   //狗叫的概率
P(B) = 2 / 20 * 365            //盗贼入侵的概率
P(A|B) = 0.9                   //盗贼入侵时候狗叫的概率
P(B|A) = P(A|B) * P(B) / P(A)  //狗叫的时候盗贼入侵的概率
P(A∩B) = P(A|B) * P(B)         //盗贼入侵狗叫同时发生的概率
```
### 条件概率
即当一个事件发生时另一个事件发生的概率 -> 当盗贼入侵(B)时狗叫(A)的概率 -> P(A|B)
### 先验概率/后验概率
- 先验概率不是根据有关自然状态的全部资料测定的，而只是利用现有的材料(主要是历史资料)计算的；
- 后验概率使用了有关自然状态更加全面的资料，既有先验概率资料，也有补充资料；
- 上面的别墅被盗问题中P(A)、P(B)都是先验概率，他们都是基于历史数据的测定；

### 联合概率
表示两个随机事件同时发生的概率 -> P(A∩B) 或 P(AB)